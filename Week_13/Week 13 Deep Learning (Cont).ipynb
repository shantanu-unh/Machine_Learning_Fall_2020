{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <h2 align=\"center\">Machine Learning</h2> \n",
    "<h3 align=\"center\">Travis Millburn<br>Fall 2020</h3> \n",
    "\n",
    "<center>\n",
    "<img src=\"../images/logo.png\" alt=\"drawing\" style=\"width: 300px;\"/>\n",
    "</center>\n",
    "\n",
    "<h3 align=\"center\">Week 13: Deep Learning (Continued)</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "1. Semester Projects: Proposals due today.  \n",
    "2. Week 12 HW moved to Week 13 HW.  Posted now.\n",
    "3. Plan / Schedule for Rest Remainder of Semester\n",
    "4. Deep Learning Concepts Continuation\n",
    "5. Lab\n",
    "6. Time given to projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects\n",
    "\n",
    "Requirements: \n",
    "1. Must be python, delivered in Jupyter notebook.\n",
    "2. Must be supervised learning problem.\n",
    "\n",
    "Rubric:\n",
    "1. Data selection interesting/challenging?\n",
    "2. Method interesting/challenging/thorough?\n",
    "3. Validation thorough, done properly?\n",
    "4. Report - well-written/understandable, explains pros/cons of method, what happened & why (can use jupyter completely if use markdown & latex).\n",
    "\n",
    "Note: some related tasks will go into participation or homework grade.\n",
    "\n",
    "Project Proposal due today.  Presentations 12/7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schedule For Remainder of Semester\n",
    "* 11/23  REMOTE  -  Last Day for New Content\n",
    "* 11/30  No Class: Thanksgiving Break\n",
    "* 12/7   Zoom Project Presentations\n",
    "* 12/14  Final Exam (online)  AND   Submission of final project files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thus far we have worked on \"shallow nets\" with just a few layers  \n",
    "* For more complicated tasks, such as classification with hundreds of buckets, we need deeper networks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron\n",
    "* Simple ANN architecture  \n",
    "* Built on TLU: Threshold Logic Unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../images/perceptron2.png\" alt=\"drawing\" style=\"width: 900px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Learning Algorithm\n",
    "\n",
    "1. choose starting $\\mathbf w$, $b$, stepsize $\\eta$.\n",
    "\n",
    "2. For each $(\\mathbf x_i, y_i)$: test $f(\\mathbf w,b, \\mathbf x_i) = y_i$?\n",
    "\n",
    " - If $f(\\mathbf w,b, \\mathbf x_i) \\neq y_i$, set $(\\mathbf w,b) = (\\mathbf w,b) + \\eta \\, y_i \\times (\\mathbf x_i,1)$\n",
    " \n",
    " \n",
    "Can view this algorithm as gradient descent (recall gradient descent from previous class) for the loss function:\n",
    "\n",
    "$$\n",
    "L(f(\\mathbf x_i), y_i) = \\max(0, -y_i \\mathbf w^T (\\mathbf x_i,1))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Functions  \n",
    "* Most common: Heavyside Step Function\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/heavyside_step_func.png\" alt=\"drawing\" style=\"width: 900px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron  \n",
    "* Input layer + layers of TLUs (we will refer to them as hidden layers) + Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../images/MLP.png\" alt=\"drawing\" style=\"width: 900px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NN that contain many many hidden layers are called Deep Neural Networks: DNN vs regular ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Backpropagation:  Gradient Descent with a clever technique for calculating the gradients\n",
    "\n",
    "* Automatically computing gradients is called automatic differentiation, or autodiff  (you will see this as a hyperparameter)  \n",
    "\n",
    "* Done in batches.  Each batch is an \"Epoch\"  -- we saw these last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation Functions\n",
    "* Activation functions can be binary, linear, or non-linear\n",
    "\n",
    "    * Sigmoid\n",
    "    * Hyperbolic Tangent\n",
    "    * ReLU (Rectified Linear Unit)\n",
    "    * Leaky ReLU\n",
    "    * Parametric ReLU\n",
    "    * Softmax\n",
    "    * Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../images/shallow.png\" alt=\"drawing\" style=\"width: 900px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../images/dnn.png\" alt=\"drawing\" style=\"width: 900px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
