{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\">Machine Learning</h2> \n",
    "<h3 align=\"center\">Travis Millburn<br>Fall 2020</h3> \n",
    "\n",
    "<center>\n",
    "<img src=\"../images/logo.png\" alt=\"drawing\" style=\"width: 300px;\"/>\n",
    "</center>\n",
    "\n",
    "<h3 align=\"center\">Class 9: Bootstrapping + Optimization + Model Bias</h3> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "1. Quick review of variance vs std-deviation\n",
    "\n",
    "2. Bootstrapping\n",
    "\n",
    "3. A Bit about Bias\n",
    "\n",
    "4. Confusion Matrix\n",
    "\n",
    "5. Lab\n",
    "\n",
    "6. Time for projects / Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Review\n",
    "\n",
    "<img src=\"../images/population_and_sample_Nn.png\" alt=\"drawing\"  width=\"30%\"  align=\"right\"/>\n",
    "    \n",
    "#### Population variance: $\\sigma^2 = \\frac{\\sum_{i=1}^N (x_i - \\mu)^2}{N}$ \n",
    "\n",
    "   ...As in _the whole population_\n",
    "\n",
    "#### Sample variance: $ s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}$ \n",
    "\n",
    "   ...As in _just the sample_\n",
    "    \n",
    "Note that sneaky $n-1$ denominator. \n",
    "\n",
    "#### Standard deviation = $\\sqrt{\\text{Variance}}$ \n",
    "\n",
    "Variance is a kind of average too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bagging\n",
    "Bagging, or bootstrap aggregation, is a model aggregation technique to reduce model variance.  Data is split into multiple samples with replacement(!) called bootstrap samples. A bootstrap sample often is 3/4 of the original values and replacement resulting in repetition of values inside each model run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"../images/bagging.jpg\" alt=\"drawing\" style=\"width: 1000px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discussion: How is this differnt than K-Fold ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### We will come back to this for today's \"Micro-Lab.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization review\n",
    "\n",
    "Find \"input\" that minimizes or maximizes some function\n",
    "\n",
    "In machine learning, what functions do we want to minimize and why? For regression?\n",
    "\n",
    "* Calculus approach\n",
    "\n",
    "* Via numerical package ~ black box\n",
    "\n",
    "  - Tricks and equivalent problems\n",
    "  - logarithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Formal Machine Learning Framework & Jargon 1/3\n",
    "\n",
    "1. Given training data $(\\mathbf x_{(i)},y_i)$ for $i=1,...,m$.\n",
    "\n",
    "2. Choose a model $f(\\cdot)$ where $f(\\mathbf x)\\approx y$\n",
    "\n",
    "3. Define a loss function $L(f(\\mathbf x), y)$ to minimize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Formal Machine Learning Framework & Jargon 2/3\n",
    "\n",
    "Have data $(\\mathbf x_{(i)},y_i)$, model $f(\\cdot)$, and loss function $L(f(\\mathbf x), y)$.\n",
    "\n",
    "Want to minimize _true loss_ which is the expected value of the loss.\n",
    "\n",
    "We approximate it by minimizing the _Empirical loss_  (a.k.a \"risk\") \n",
    "$$\n",
    "L_{emp}(f) = \\frac{1}{m}\\sum_{i=1}^m L\\big(f(\\mathbf x_{(i)}), y_i\\big)\n",
    "$$\n",
    "Emirical Risk minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Formal Machine Learning Framework & Jargon 3/3\n",
    "\n",
    "Emirical Risk minimization:\n",
    "$$\n",
    "f^* = \\arg \\min\\limits_f L_{emp}(f) = \\arg \\min\\limits_f\\frac{1}{m}\\sum_{i=1}^m L\\big(f(\\mathbf x_{(i)}), y_i\\big)\n",
    "$$\n",
    "To trade-off model risk and simplicity, we include regularizer:\n",
    "$$\n",
    "f^* = \\arg \\min\\limits_f \\big( L_{emp}(f) + \\lambda R(f) \\big) = \\arg \\min\\limits_f  L_{reg}(f)\n",
    "$$\n",
    "\n",
    "Just cryptic talk for the same optimization problem we've been doing. But can be extended to cover many different techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "\n",
    "Have data $(\\mathbf x_{(i)},y_i)$, model $f(\\cdot)$, and loss function $L(f(\\mathbf x), y)$.\n",
    "\n",
    "Regularized Emirical Risk minimization:\n",
    "$$\n",
    "f^* = \\arg \\min\\limits_f \\big( L_{emp}(f) + \\lambda R(f) \\big) = \\arg \\min\\limits_f  L_{reg}(f)\n",
    "$$\n",
    "\n",
    "Can be extended to cover classification techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias in 100 words or less\n",
    "\n",
    "Bias is when your assumptions about the true predictor affect the estimated predictor. \n",
    "\n",
    "Example: you assume it is linear, use a linear model, get a linear result. If the true function is nonlinear, this linear model will have an error due to the bias.\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/Under-fitting.png\" alt=\"drawing\" style=\"width: 400px;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "Formal definition: $$\\text{Bias} = E(y - f(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance even simpler\n",
    "\n",
    "Variance is when your model changes when fit using different sample from same population.\n",
    "\n",
    "I.e. it fits the noise $\\varepsilon$ in addition to the true function\n",
    "\n",
    "Out of fear of bias, we use a nonlinear model with way more parmeters to fit. It fits the (noisy) training data too well, and is worse than necessary on test data with different noise.\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/Overfitted_Data.png\" alt=\"drawing\" style=\"width: 400px;\"/>\n",
    "</center>\n",
    "\n",
    "Formal definition: $$\\text{Variance} = Var(f(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias-Variance Trade-off\n",
    "\n",
    "\n",
    "Life is hard.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/bias_var_tradeoff.JPG\" alt=\"drawing\" style=\"width: 700px;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "But then this is why Machine Learning experts make the big bucks.\n",
    "\n",
    "Regularization as restriction on model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example  \n",
    "\n",
    "Let's start by looking at an example. We're going to be using some NFL data. The x axis is the number of touchdowns scored by team over a season and the y axis is whether they lost or won the game indicated by a value of 0 or 1 respectively.\n",
    "\n",
    "<center><img src=\"../images/nfl.png\" alt=\"drawing\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "So, how do we predict whether we have a win or a loss if we are given a score? Note that we are going to be predicting values between 0 and 1. Close to 0 means we're sure it's in class 0, close to 1 means we're sure it's in class 1, and closer to 0.5 means we don't know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring success \n",
    "\n",
    "So how do we measure how well our model does? \n",
    "\n",
    "1. how do we test _generalizibility_?\n",
    "\n",
    "2. What specific metrics might we compute?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (exam review)\n",
    "The simplest measure is **accuracy**. This is the number of correct predictions over the total number of predictions. It's the percent you predicted correctly. In `sklearn`, this is what the `score` method calculates.\n",
    "\n",
    "#### Shortcomings\n",
    "\n",
    "Accuracy is a good first glance measure, but it has shortcomings. \n",
    "\n",
    "If the classes are unbalanced, accuracy will not measure how well you did at predicting. Say you are trying to predict whether or not an email is spam. Only 2% of emails are in fact spam emails. You could get 98% accuracy by always predicting not spam. This is a great accuracy but a horrible model!\n",
    "\n",
    "What additional measurements might we do to check for such failures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "We can get a better picture our model by looking at the confusion matrix. We get the following four metrics:\n",
    "\n",
    "* **True Positives (TP)**: Correct positive predictions\n",
    "* **False Positives (FP)**: Incorrect positive predictions (false alarm)\n",
    "* **True Negatives (TN)**: Correct negative predictions\n",
    "* **False Negatives (FN)**: Incorrect negative predictions (a miss)\n",
    "\n",
    "<center><img src=\"../images/logistic.png\" alt=\"drawing\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "Note what happens as you move the decision threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging!  Let's look at an example\n",
    "\n",
    "(Bootstrap Aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports -- usual suspects\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling\n",
    "col_names = [\n",
    "    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n",
    "    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n",
    "    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv('adult.csv')\n",
    "df.columns=col_names\n",
    "df['Income'] = df['Income'].apply(lambda x: 0 if x == ' <=50K' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some (very) light feature engineering\n",
    "def age_bucket(x):\n",
    "\n",
    "    if 12 <= x < 18:\n",
    "        return 1\n",
    "    elif 18 <= x < 30:\n",
    "        return 2\n",
    "    elif 30 <= x < 40:\n",
    "        return 3\n",
    "    elif 40 <= x < 50:\n",
    "        return 4\n",
    "    elif 50 <= x < 66:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6\n",
    "    \n",
    "    \n",
    "def education_bucket(x):\n",
    "    if x in [' 1st-4th', ' 5th-6th', ' 7th-8th' , ' 9th',  ' 10th', ' 11th', ' 12th', ' Preschool']:\n",
    "        return 1\n",
    "    elif x in [ ' HS-grad']:\n",
    "        return 2\n",
    "    elif x in [' Some-college']:\n",
    "        return 3\n",
    "    elif x in [ ' Bachelors']:\n",
    "        return 5\n",
    "    elif x in [' Masters']:\n",
    "        return 6\n",
    "    elif x in [' Assoc-acdm', ' Assoc-voc']:\n",
    "        return 4\n",
    "    elif x in [' Doctorate', ' Prof-school']:\n",
    "        return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b692b4b65848>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CapitalNet'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CapitalGain'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CapitalLoss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age_class'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage_bucket\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Education_Class'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Education'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meducation_bucket\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['CapitalNet'] = df['CapitalGain'] - df['CapitalLoss']\n",
    "df['Age_class'] = df['Age'].apply(age_bucket)\n",
    "df['Education_Class'] = df['Education'].apply(education_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>WorkClass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationNum</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Gender</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>NativeCountry</th>\n",
       "      <th>Income</th>\n",
       "      <th>CapitalNet</th>\n",
       "      <th>Age_class</th>\n",
       "      <th>Education_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32555</th>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>257302</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>154374</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>151910</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>201490</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>287927</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>15024</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  WorkClass  fnlwgt  Education  EducationNum  MaritalStatus  \\\n",
       "32555   27          4  257302          7            12              2   \n",
       "32556   40          4  154374         11             9              2   \n",
       "32557   58          4  151910         11             9              6   \n",
       "32558   22          4  201490         11             9              4   \n",
       "32559   52          5  287927         11             9              2   \n",
       "\n",
       "       Occupation  Relationship  Race  Gender  CapitalGain  CapitalLoss  \\\n",
       "32555          13             5     4       0            0            0   \n",
       "32556           7             0     4       1            0            0   \n",
       "32557           1             4     4       0            0            0   \n",
       "32558           1             3     4       1            0            0   \n",
       "32559           4             5     4       0        15024            0   \n",
       "\n",
       "       HoursPerWeek  NativeCountry  Income  CapitalNet  Age_class  \\\n",
       "32555            38             39       0           0          2   \n",
       "32556            40             39       1           0          4   \n",
       "32557            40             39       0           0          5   \n",
       "32558            20             39       0           0          2   \n",
       "32559            40             39       1       15024          5   \n",
       "\n",
       "       Education_Class  \n",
       "32555                4  \n",
       "32556                2  \n",
       "32557                2  \n",
       "32558                2  \n",
       "32559                2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].dtype == type(object):\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Income'])\n",
    "y = df['Income']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=555, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remember from a few weeks ago how to build a Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81470106, 0.81081081, 0.81101556, 0.80835381, 0.80487305])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-Fold\n",
    "results = sklearn.model_selection.cross_val_score(dt, X_train,y_train, cv=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8099508599508599, 0.003248760601195561)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequently, we will want to average the models across all the folds\n",
    "results.mean(), results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8520884520884521"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BAGGING\n",
    "# Create a bag of estimators of size 11\n",
    "dt_bag = BaggingClassifier(base_estimator=dt, n_estimators=100, random_state=555, n_jobs=-1)\n",
    "\n",
    "# Fit / Train model\n",
    "dt_bag.fit(X_train,y_train)\n",
    "\n",
    "#Results\n",
    "results = dt_bag.score(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This comparison illustrates:\n",
    "* K-fold cross validation divides data into buckets and models on all, holding out one.  K-number of models.\n",
    "* Bagging creates N number of models and does a train/test split (with replacement) for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use this bagging method with other (any) base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7878378378378378"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "nn_bag = BaggingClassifier(base_estimator=knn, n_estimators=100, random_state=555, n_jobs=-1)\n",
    "nn_bag.fit(X_train,y_train)\n",
    "\n",
    "#Results\n",
    "results = nn_bag.score(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Next Week: Boosting!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Time for week 9 lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
